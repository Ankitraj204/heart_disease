# -*- coding: utf-8 -*-
"""heartDisease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GpOlhUMv49xdjjyOxyiHyzFCjLt8WG3F
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("framingham.csv")
data.head()

from sklearn.model_selection import train_test_split
X = data.iloc[:,:-1]
y = data.iloc[:,-1]
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1,test_size=0.25,shuffle=True)
train_data = pd.concat([X_train,y_train],axis=1)
test_data = pd.concat([X_test,y_test],axis=1)

"""**Missing Values** Checking Missing values can be done before EDA or after EDA. But before EDA, it will impute or drop missing values for all features, whether some features are needed or not

And after EDA, we choose the features which are needed and those features only get imputed.

Also, the steps best for model preparation is : EDA -> Preprocessing (Missing values, Outliers, Normalise etc.) -> Model Fitting and Prediction

Maximum missing percentage is 9% approx so imputation will be done
"""

# Resampling

# gives an array of number of null values in eacj col
missing_values_count = train_data.isnull().sum()
# data.shape[0] gives the number of rows
missing_values_percent = (missing_values_count * 100) / (train_data.shape[0])
#print(max(missing_values_percent))

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent')

new_data = pd.DataFrame(imputer.fit_transform(train_data))
new_data.head()
new_data.columns = train_data.columns
new_data.index = train_data.index

train_data.isnull().sum()

train_data = new_data.copy()
train_data.isnull().sum()

"""**RESAMPLING**"""

from imblearn.over_sampling import SMOTE
binary_feature_indices = [0,5,6,7,8]
X_train = train_data.iloc[:,:-1]
y_train = train_data.iloc[:,-1]
smote = SMOTE(sampling_strategy='auto',random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train,y_train)
X_train_os = pd.DataFrame(X_resampled)
X_train_os.iloc[:, binary_feature_indices] = np.round(X_train_os.iloc[:, binary_feature_indices])
y_resampled = np.round(y_resampled)
y_train_os = pd.DataFrame(y_resampled)
train_data = pd.concat([X_train_os,y_train_os],axis=1)
train_data["TenYearCHD"].value_counts()
train_data.head()

"""**Train_data**"""

sns.set_style('whitegrid')
sns.countplot(x='TenYearCHD',data=train_data,palette='RdBu_r')
train_data["TenYearCHD"].value_counts()

train_data.head()

train_data.info()

train_data.describe()

# education feature is not required as its not predicting the Ten Year CHD
# target is Ten Year CHD (0 or 1)
# inplace is true means removing is done on the current dataframe
# aixs = 1 means columns
train_data.drop('education',axis=1, inplace=True)

# renaming TenYearCHD to CHD
train_data.rename(columns={"TenYearCHD": "CHD"}, inplace=True)

train_data.head()

"""**EDA**"""

# age v/s CHD
plt.figure(figsize=(8,8))
sns.swarmplot(x='CHD', y='age', data=train_data)

plt.figure(figsize=(10,10))
sns.violinplot(x='CHD', y='age', data=train_data)

"""violin plot tells that most patients of age around 40-55 have risk 0



Most patients of age around 60-65 have risk of disease(CHD)
"""

# age vs CHD for smokers or non- smokers
plt.figure(figsize=(10,10))
sns.swarmplot(x='CHD', y='age', data=train_data, hue='currentSmoker')

"""for the age group of 40-50 most of the patients are smokers."""

#plt.figure(figsize=(10,10))
#sns.violinplot(x='CHD', y='age', data=train_data, hue='currentSmoker', split=True)

"""If we look upto the peaks

We see most of the smokers having no risk of CHD in age around 40yrs

Most of non-smokers having risk are in age group 65-70 yrs

Also most of the smokers having risk are in age aroung 50yrs

for the smokers who are in 40s whiil get risk of CHD in their 50s
"""

# male and female countplot
sns.countplot(x=train_data['male'])

# male and female having disease or not
sns.countplot(x=train_data['male'], hue=train_data['CHD'])

"""From above countplot

Most of the data are for females

More females having no risk than males having no risk

There are slightly more males having risk than females havinf risk
"""

train_data.iloc[:,:5]

# To understand correlation between some features, pair plot is used
plt.figure(figsize=(20,15))
sns.pairplot(train_data.loc[:,'totChol':'glucose'])

plt.figure(figsize=(15,15))
sns.heatmap(train_data.corr(), annot=True, linewidth=0.1)

# dropping features which are highly correlated
features_to_drop = ['currentSmoker', 'diaBP']

train_data.drop(features_to_drop, axis=1, inplace = True)
train_data.head()

"""**Outliers Checking**"""

fig, ax = plt.subplots(figsize=(10,10), nrows=3, ncols=4)
ax = ax.flatten()

i=0
for k,v in train_data.items():
  sns.boxplot(y=v, ax=ax[i])
  i+=1
  if i==12:
    break
plt.tight_layout(pad=1.25, h_pad=0.8, w_pad=0.8)

"""Conclusion of BoxPlot:

Outliers found in features named['totChol','sysBP','BMI','heartRate','glucose']
"""

#Outliers handling
print('Number of training examples to deleteed for outliers removal is ',len(data[data['sysBP']>220]) + len(data[data['heartRate']>125]) + len(data[data['glucose']>200]) + len(data[data['totChol']>450]))

# deleting outliers

train_data = train_data[~(train_data['sysBP']>220)]
train_data = train_data[~(train_data['BMI']>43)]
train_data = train_data[~(train_data['heartRate']>225)]
train_data = train_data[~(train_data['glucose']>200)]
train_data = train_data[~(train_data['totChol']>450)]
print(train_data.shape)

"""**NORMALIZATION CHECKING**"""

fig, ax = plt.subplots(figsize=(10,10), nrows=3, ncols=4)
ax = ax.flatten() # plot size and orientation

i = 0
for k,v in train_data.items():
    sns.distplot(v, ax=ax[i])
    i+=1
    if i==12:
        break
plt.tight_layout(pad=1.25, h_pad=0.8, w_pad=0.8)

# Standardise some features

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
cols_to_standardise = ['age','totChol','sysBP','BMI','heartRate','glucose','cigsPerDay']
train_data1 = train_data.copy();
train_data1[cols_to_standardise] = scaler.fit_transform(train_data1[cols_to_standardise])

train_data.head()

"""**Test_data**"""

# dropping unwanted features as done in train data
test_data.drop(["education"],axis=1,inplace=True)
test_data.drop(features_to_drop, axis=1, inplace=True)
test_data.rename(columns={"TenYearCHD": "CHD"}, inplace=True)
# imputing missing values if any
imputer = SimpleImputer(strategy='most_frequent')
new_test_data = pd.DataFrame(imputer.fit_transform(test_data))
new_test_data.columns = test_data.columns
new_test_data.index = test_data.index

test_data = new_test_data.copy()

# Standardising features
scaler1 = StandardScaler()
test_data1 = test_data.copy()
test_data1[cols_to_standardise] = scaler.fit_transform(test_data1[cols_to_standardise])

test_data1.head()

"""**LOGISTIC REGRESSION**"""

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()

X_train = train_data1.iloc[:,:-1]
y_train = train_data1.iloc[:,-1]
X_test = test_data1.iloc[:,:-1]
y_test = test_data1.iloc[:,-1]

log_reg.fit(X_train,y_train)
y_pred_log = log_reg.predict(X_test)

data_test_1=[[1,65,1,1,1,1,1,195,106,21.02,80,77]]
# Standardising features
# scaler = StandardScaler()
# data_test_1 = scaler.fit_transform(data_test_1)
# print(data_test_1)

print(log_reg.predict(data_test_1)[0])

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

log_reg_accuracy = accuracy_score(y_pred_log, y_test) * 100
print('Accuracy Score for logistic regression is %f'%log_reg_accuracy)

log_train_score = log_reg.score(X_train,y_train)*100
print('Train score for Logistic Regression is %f'%log_train_score)

print('Difference between train and test score for Logistic Regression is %f'%(log_train_score - log_reg_accuracy))

confusion_matrix(y_pred_log,y_test)

print(classification_report(y_pred_log, y_test))

"""**DECISION TREE CLASSIFIER**"""

from sklearn.tree import DecisionTreeClassifier

X_train = train_data.iloc[:,:-1]
y_train = train_data.iloc[:,-1]
X_test = test_data.iloc[:,:-1]
y_test = test_data.iloc[:,-1]

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
dt_classifier = DecisionTreeClassifier()

# Define hyperparameters and their respective distributions for random search
param_dist = {
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(1, 9),  # Random integer values from 1 to 9
    'min_samples_split': randint(2, 200),  # Random integer values from 2 to 200
    'min_samples_leaf': randint(1, 100)  # Random integer values from 1 to 100
}

# Use RandomizedSearchCV for hyperparameter tuning
random_search = RandomizedSearchCV(dt_classifier, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_

# Train the final model with the best hyperparameters
final_model = DecisionTreeClassifier(**best_params)
final_model.fit(X_train, y_train)

# Evaluate the final model on the test set
y_pred_dt = final_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_dt)

print("Best Hyperparameters:", best_params)
print("Accuracy on Test Set:", accuracy)

print(final_model.predict([[1,18,0,0,0,0,0,195,106,21.02,80,77]])[0])

dt_accuracy= accuracy_score(y_pred_dt, y_test)*100
print('Accuracy score for Decision tree is %f'%dt_accuracy)

dt_train_score = final_model.score(X_train, y_train)*100
print('Train score for Decision tree is %f'%dt_train_score)

print('Difference between train and test scores for Decision tree is %f'%(dt_train_score - dt_accuracy))

confusion_matrix(y_pred_dt, y_test)

print(classification_report(y_pred_dt,y_test))

"""**RANDOM FOREST CLASSIFIER**"""

from sklearn.ensemble import RandomForestClassifier

X_train = train_data.iloc[:,:-1]
y_train = train_data.iloc[:,-1]
X_test = test_data.iloc[:,:-1]
y_test = test_data.iloc[:,-1]

rf_classifier = RandomForestClassifier()

# Define hyperparameters and their respective distributions for random search
param_dist = {
    'n_estimators': randint(10, 200),  # Random integer values from 10 to 200
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(1, 9),
    'min_samples_split': randint(2, 200),
    'min_samples_leaf': randint(1, 100),
    'max_features': ['auto', 'sqrt', 'log2', None]
}

# Use RandomizedSearchCV for hyperparameter tuning
random_search_rf = RandomizedSearchCV(rf_classifier, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_rf.fit(X_train, y_train)

# Get the best hyperparameters
best_params_rf = random_search.best_params_

# Train the final model with the best hyperparameters
final_model_rf = RandomForestClassifier(**best_params_rf)
final_model_rf.fit(X_train, y_train)

# Evaluate the final model on the test set
y_pred_rf = final_model_rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_rf)

print("Best Hyperparameters:", best_params_rf)
print("Accuracy on Test Set:", accuracy)

final_model_rf.fit(X_train, y_train)
y_pred_rf = final_model_rf.predict(X_test)

rf_accuracy = accuracy_score(y_pred_rf, y_test)*100
print('Accuracy score for Random Forest is %f'%rf_accuracy)

rf_train_score = final_model_rf.score(X_train, y_train)*100
print('Train score for Random Forest is %f'%rf_train_score)

print('Difference between train and test scores for Random Forest is : %f'%(rf_train_score - rf_accuracy))

confusion_matrix(y_pred_rf, y_test)

print(classification_report(y_pred_rf, y_test))

"""**Ensemble_model**"""

import xgboost as xgb

X_train = train_data.iloc[:,:-1]
y_train = train_data.iloc[:,-1]
X_test = test_data.iloc[:,:-1]
y_test = test_data.iloc[:,-1]

model = xgb.XGBClassifier(objective="multi:softmax", num_class=len(set(y)), random_state=42)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Display classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""**CONCLUSION**

Why to choose an algorithm
"""